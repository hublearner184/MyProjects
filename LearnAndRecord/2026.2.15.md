# 2026.2.15

## 翻转和旋转

### RandomHorizontalFlip(p)

功能：依概率水平翻转

p：以概率p选择翻转或者不翻转

### RandomVerticalFlip(p)

### RandomRotation

RandomRotation(degrees,resample=False,expand=False,center=None)

功能：随机旋转图片         degrees：旋转角度  当为a时，在（-a，a）之间选择旋转角度

为（a，b）时在（a，b）之间选择旋转角度    resample：重采样方法 expand：是否扩大图片，以保持原图完整  center：旋转点设置，默认中心旋转

### Pad

功能：对图片边缘进行填充   padding：设置填充大小 当为a时，上下左右均填充a个像素

当为（a，b）时左右填充a个像素，上下填充b个像素

padding_mode：填充模式，有4种模式，constant、edge、reflect和symmetric

fill：constant时，设置填充的像素值（R,G,B）or（Gray）

### ColorJitter

transforms.ColorJitter(brightness=0,contrast=0,saturation=0,hue=0)

功能：调整亮度、对比度、饱和度和色相

brightness：亮度调整因子当为a时，从[max(0,1-a),1+a]中随机选择  当为（a，b）时从[a,b]中随机选择

saturation：饱和度参数  hue：色相参数，当为a时，从[-a,a]中选择参数 注：0<=a<=0.5

当为（a，b）时，从[a,b]中选择参数  注:-0.5<=a<=b<=0.5

### Grayscale(num_output_channels)与RandomGrayscale(num_output_channels,p)

功能：依概率将图片转换为灰度图

num_output_channels：输出通道数

只能设置1或3  设置3时R=G=B

p：概率值，图像被转换为灰度图的概率

### RandomAffine

RandomAffine(degrees,translate=None,scale=None,shear=None,resample=False,fillcolor=0)  degrees:旋转角度设置   translate:平移区间设置   scale:缩放比例（以面积为单位）       fill_color:填充颜色设置  shear：错切角度设置，有水平错切和垂直错切 若为a，仅在x轴错切，错切角度在（-a，a）之间，若为（a，b），则a设置x轴角度，b设置y轴角度  resample：重采样方式，有NEAREST、BILINEAR、BICUBIC

功能：对图像进行仿射变换仿射变换是二维的线性变换，由五种基本原子变换构成，分别是旋转、平移、缩放、错切和翻转

### RandomErasing

RandomErasing(p=0.5,scale(0.02,0.33),ratio(0.3,3.3),value=0,inplace=False)

功能：对图像进行随机遮挡

p：概率值，执行该操作的概率   scale：遮挡区域的面积    ratio：遮挡区域长宽比  value：设置遮挡区域的像素值

作用在Tensor上

### transforms.Lambda

功能：用户自定义lambda方法     lambda：lambda匿名函数

## transforms方法组合

### transforms.RandomChoice

功能：从一系列transforms方法中随机挑选一个

transforms.RandomChoice([transforms1,transforms2,transforms3])

### transforms.RandomApply

功能：依据概率执行一组transforms操作

### transforms.RandomOrder

功能：以随机顺序运行一组transforms

transforms.RandomOrder([transforms1,transforms2,transforms3])

## 自定义transforms

1.仅接收一个参数，返回一个参数

2.注意上下游的输出与输入

## 网络模型创建步骤

### 模型构建三要素

![image-20260215210650170](C:\Users\nickname\AppData\Roaming\Typora\typora-user-images\image-20260215210650170.png)

1.定义各种layer __init__()

2.逐层计算layer forward()

3.layer参数初始化 initialize_weights()

### nn.Module

import torch.nn

包括nn.Parameter张量子类，表示可学习参数，如weight，bias

nn.Module 所有网络层基类，管理网络属性

nn.functional 函数具体实现，如卷积，池化，激活函数等

nn.init 参数初始化方法

## 层容器（Containers）

### nn.Sequential

按顺序组装、并运行多个网络层

是nn.module的容器，用于按顺序包装一组网络层

### nn.ModuleList

像python的list一样组装多个网络层

### nn.ModuleDict

像python的dict一样组装多个网络层

# 2026.2.16

## 容器之ModuleList

### nn.ModuleList是nn.module的容器

用于包装一组网络层，以迭代的方式调用网络层 

主要方法：

append() 在ModuleList后面添加网络层

extend() 拼接两个ModuleList

insert() 指定在ModuleList中位置插入网络层

### nn.ModuleDict

## AlexNet（ImageNet ）

AlexNet特点：

1.采用ReLU：替换饱和激活函数，减轻梯度损失

2.采用LRU（Local Response Normalization）：对数据进行归一化，减轻梯度损失

3.Dropout：提高全连接层的鲁棒性，增强网络的泛化能力

4.Data Augmentation：TenCrop，色彩修改

## 卷积层

### 1，2，3d Convolution

卷积运算：卷积核在输入信号（图像）上滑动，相应位置上进行乘加

卷积核：又称为滤波器，过滤器，可以被认为是某种模式，某种特征。

卷积过程类似于用一个模板去图像上寻找与它相似的区域，与卷积核模式越相似，激活值越高，从而实现特征提取。

AlexNet卷积核可视化，发现卷积核学习到的是边缘，条纹，色彩这一些细节模式。

卷积维度：一般情况下，卷积核在几个维度上滑动就是几维卷积

### nn.Conv2d

nn.Conv2d(in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode=‘zeros’)

功能：对多个二维信号进行二维卷积

输入通道数，输出通道数，等价于卷积核个数，卷积核尺寸，步长，填充个数，空洞卷积大小，分组卷积设置，偏置

简化版outsize = Insize - kernelsize/stride +1

### 转置卷积

转置卷积又称为反卷积和部分跨越卷积用于对图像进行上采样

nn.ConvTranspose2d

上采样outsize = （insize - 1）* stride + kernelsize   与下采样相反

## 池化层（pooling Layer）

池化运算：对信号进行“收集”并“总结”，类似水池收集水资源，因而得名池化层

收集：多变少  总结：最大值/平均值

nn.MaxPool2d

功能：对二维信号进行最大值池化

主要参数：kernel_size池化核尺寸 stride  padding  dilation ceil_mode：输出shape向上取整  return_indices 记录池化像素索引

降维，去噪，提取最显著特征

nn.AvgPool2d

功能：对二维信号进行最大值池化

主要参数：kernel_size 池化核尺寸 stride 步长 padding填充个数 ceil_mode 输出shape向上取整   count_include_pad 平均时是否考虑pad的0  divisor_override:除法因子

nn.MaxUnpool2d

功能：对二维信号进行最大值池化上采样

主要参数：kernel_size:池化核尺寸  stride：步长  padding：填充个数

## 线性层（Linear Layer）

又称为全连接层，其每个神经元与上一层所有神经元相连实现对前一层的线性组合，线性变换主要参数：  input  hidden  output

nn.Linear   功能：对一维信号进行线性组合   主要参数：in_features：输入结点数  out_features:输出结点数  bias：是否需要偏置    y= xWT + bias

起到维度变换的作用

## 激活函数层（Activation Layer）

激活函数对特征进行非线性变换，赋予多层神经网络具有深度的意义

### Sigmoid（）

特性：输出在（0，1）符合概率；导数范围是在[0,0.25]易导致梯度消失；输出为非0均值，破坏数据分布

### tanh（）

特性：输出值在（-1，1），数据符合0均值；导数范围是（0，1），易导致梯度消失

### ReLU（）

y=max（0，x）

特性：输出值均为正数，负半轴导致死神经元；导数是1，缓解梯度消失，但易引发梯度爆炸


# 2026.2.15

## 翻转和旋转

### RandomHorizontalFlip(p)

功能：依概率水平翻转

p：以概率p选择翻转或者不翻转

### RandomVerticalFlip(p)

### RandomRotation

RandomRotation(degrees,resample=False,expand=False,center=None)

功能：随机旋转图片         degrees：旋转角度  当为a时，在（-a，a）之间选择旋转角度

为（a，b）时在（a，b）之间选择旋转角度    resample：重采样方法 expand：是否扩大图片，以保持原图完整  center：旋转点设置，默认中心旋转

### Pad

功能：对图片边缘进行填充   padding：设置填充大小 当为a时，上下左右均填充a个像素

当为（a，b）时左右填充a个像素，上下填充b个像素

padding_mode：填充模式，有4种模式，constant、edge、reflect和symmetric

fill：constant时，设置填充的像素值（R,G,B）or（Gray）

### ColorJitter

transforms.ColorJitter(brightness=0,contrast=0,saturation=0,hue=0)

功能：调整亮度、对比度、饱和度和色相

brightness：亮度调整因子当为a时，从[max(0,1-a),1+a]中随机选择  当为（a，b）时从[a,b]中随机选择

saturation：饱和度参数  hue：色相参数，当为a时，从[-a,a]中选择参数 注：0<=a<=0.5

当为（a，b）时，从[a,b]中选择参数  注:-0.5<=a<=b<=0.5

### Grayscale(num_output_channels)与RandomGrayscale(num_output_channels,p)

功能：依概率将图片转换为灰度图

num_output_channels：输出通道数

只能设置1或3  设置3时R=G=B

p：概率值，图像被转换为灰度图的概率

### RandomAffine

RandomAffine(degrees,translate=None,scale=None,shear=None,resample=False,fillcolor=0)  degrees:旋转角度设置   translate:平移区间设置   scale:缩放比例（以面积为单位）       fill_color:填充颜色设置  shear：错切角度设置，有水平错切和垂直错切 若为a，仅在x轴错切，错切角度在（-a，a）之间，若为（a，b），则a设置x轴角度，b设置y轴角度  resample：重采样方式，有NEAREST、BILINEAR、BICUBIC

功能：对图像进行仿射变换仿射变换是二维的线性变换，由五种基本原子变换构成，分别是旋转、平移、缩放、错切和翻转

### RandomErasing

RandomErasing(p=0.5,scale(0.02,0.33),ratio(0.3,3.3),value=0,inplace=False)

功能：对图像进行随机遮挡

p：概率值，执行该操作的概率   scale：遮挡区域的面积    ratio：遮挡区域长宽比  value：设置遮挡区域的像素值

作用在Tensor上

### transforms.Lambda

功能：用户自定义lambda方法     lambda：lambda匿名函数

## transforms方法组合

### transforms.RandomChoice

功能：从一系列transforms方法中随机挑选一个

transforms.RandomChoice([transforms1,transforms2,transforms3])

### transforms.RandomApply

功能：依据概率执行一组transforms操作

### transforms.RandomOrder

功能：以随机顺序运行一组transforms

transforms.RandomOrder([transforms1,transforms2,transforms3])

## 自定义transforms

1.仅接收一个参数，返回一个参数

2.注意上下游的输出与输入

## 网络模型创建步骤

### 模型构建三要素

![image-20260215210650170](C:\Users\nickname\AppData\Roaming\Typora\typora-user-images\image-20260215210650170.png)

1.定义各种layer __init__()

2.逐层计算layer forward()

3.layer参数初始化 initialize_weights()

### nn.Module

import torch.nn

包括nn.Parameter张量子类，表示可学习参数，如weight，bias

nn.Module 所有网络层基类，管理网络属性

nn.functional 函数具体实现，如卷积，池化，激活函数等

nn.init 参数初始化方法

## 层容器（Containers）

### nn.Sequential

按顺序组装、并运行多个网络层

是nn.module的容器，用于按顺序包装一组网络层

### nn.ModuleList

像python的list一样组装多个网络层

### nn.ModuleDict

像python的dict一样组装多个网络层

# 2026.2.16

## 容器之ModuleList

### nn.ModuleList是nn.module的容器

用于包装一组网络层，以迭代的方式调用网络层 

主要方法：

append() 在ModuleList后面添加网络层

extend() 拼接两个ModuleList

insert() 指定在ModuleList中位置插入网络层

### nn.ModuleDict

## AlexNet（ImageNet ）

AlexNet特点：

1.采用ReLU：替换饱和激活函数，减轻梯度损失

2.采用LRU（Local Response Normalization）：对数据进行归一化，减轻梯度损失

3.Dropout：提高全连接层的鲁棒性，增强网络的泛化能力

4.Data Augmentation：TenCrop，色彩修改

## 卷积层

### 1，2，3d Convolution

卷积运算：卷积核在输入信号（图像）上滑动，相应位置上进行乘加

卷积核：又称为滤波器，过滤器，可以被认为是某种模式，某种特征。

卷积过程类似于用一个模板去图像上寻找与它相似的区域，与卷积核模式越相似，激活值越高，从而实现特征提取。

AlexNet卷积核可视化，发现卷积核学习到的是边缘，条纹，色彩这一些细节模式。

卷积维度：一般情况下，卷积核在几个维度上滑动就是几维卷积

### nn.Conv2d

nn.Conv2d(in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode=‘zeros’)

功能：对多个二维信号进行二维卷积

输入通道数，输出通道数，等价于卷积核个数，卷积核尺寸，步长，填充个数，空洞卷积大小，分组卷积设置，偏置

简化版outsize = Insize - kernelsize/stride +1

### 转置卷积

转置卷积又称为反卷积和部分跨越卷积用于对图像进行上采样

nn.ConvTranspose2d

上采样outsize = （insize - 1）* stride + kernelsize   与下采样相反

## 池化层（pooling Layer）

池化运算：对信号进行“收集”并“总结”，类似水池收集水资源，因而得名池化层

收集：多变少  总结：最大值/平均值

nn.MaxPool2d

功能：对二维信号进行最大值池化

主要参数：kernel_size池化核尺寸 stride  padding  dilation ceil_mode：输出shape向上取整  return_indices 记录池化像素索引

降维，去噪，提取最显著特征

nn.AvgPool2d

功能：对二维信号进行最大值池化

主要参数：kernel_size 池化核尺寸 stride 步长 padding填充个数 ceil_mode 输出shape向上取整   count_include_pad 平均时是否考虑pad的0  divisor_override:除法因子

nn.MaxUnpool2d

功能：对二维信号进行最大值池化上采样

主要参数：kernel_size:池化核尺寸  stride：步长  padding：填充个数

## 线性层（Linear Layer）

又称为全连接层，其每个神经元与上一层所有神经元相连实现对前一层的线性组合，线性变换主要参数：  input  hidden  output

nn.Linear   功能：对一维信号进行线性组合   主要参数：in_features：输入结点数  out_features:输出结点数  bias：是否需要偏置    y= xWT + bias

起到维度变换的作用

## 激活函数层（Activation Layer）

激活函数对特征进行非线性变换，赋予多层神经网络具有深度的意义

### Sigmoid（）

特性：输出在（0，1）符合概率；导数范围是在[0,0.25]易导致梯度消失；输出为非0均值，破坏数据分布

### tanh（）

特性：输出值在（-1，1），数据符合0均值；导数范围是（0，1），易导致梯度消失

### ReLU（）

y=max（0，x）

特性：输出值均为正数，负半轴导致死神经元；导数是1，缓解梯度消失，但易引发梯度爆炸

# 2026.2.17

大年初一休

# 2026.2.18

## 权值初始化

一个好的权值初始化会加速模型的收敛

### 梯度消失与梯度爆炸

xavier和kaiming权重

Xavier初始化：

方差一致性：保持数据尺度维持在恰当范围内，通常方差为1    激活函数：饱和函数如Sigmoid，Tanh

这是一个非常深刻的数学直觉问题。如果不理解“方差保持”，你就无法真正理解深度神经网络为什么能“深”。

简单来说，**“方差”在这里代表的是“信息量”或者“信号的幅度”。**

**方差保持（Variance Preservation）的核心思想就是：**

我们希望数据（信号）在穿过神经网络的每一层时，**既不要因为“声音太小”而消失（方差变0），也不要因为“声音太大”而失真（方差爆炸）。**

为了让你彻底明白，我们从**直观物理**和**数学推导**两个层面来拆解。

------

1. 直观物理：多米诺骨牌与扩音器

想象你的深度神经网络是一个由 **100 个扩音器** 串联起来的系统。

- **输入信号**：你的声音（比如电池电压数据，方差为 1）。
- **权重（Weights）**：每一个扩音器的音量旋钮。

 情况 A：方差缩小 (Variance Diminishing)

如果每个扩音器的旋钮都调得稍微小一点（比如放大倍数是 0.9）：

- 第 1 层：$0.9$
- 第 2 层：$0.9 \times 0.9 = 0.81$
- ...
- 第 100 层：$0.9^{100} \approx 0.000026$
- **结果**：信号变成了**死一般的寂静**。最后输出全是 0，没有任何信息，梯度也没法传回来（梯度消失）。

 情况 B：方差放大 (Variance Exploding)

如果每个扩音器的旋钮都调得稍微大一点（比如放大倍数是 1.1）：

- 第 1 层：$1.1$
- 第 2 层：$1.1 \times 1.1 = 1.21$
- ...
- 第 100 层：$1.1^{100} \approx 13780$
- **结果**：声音震耳欲聋，甚至把扩音器震坏了（数值溢出，变成 NaN）。或者，信号落入了激活函数的**饱和区**（比如 Sigmoid 的两端），导致梯度为 0。

情况 C：方差保持 (The "Goldilocks" Zone)

为了让第 100 个扩音器发出的声音和第 1 个扩音器一样清晰：

**中间每一个扩音器的放大倍数，必须严格等于 1。**

这就是 **Xavier / Kaiming 初始化** 想做的事：**让每一层的输出方差，严格等于输入方差。**

$$Var(\text{Output}) = Var(\text{Input})$$

------

2. 数学推导：为什么方差会变？

让我们看一层最简单的全连接层运算（忽略偏置，因为它不影响方差的缩放）：

$$y = w_1 x_1 + w_2 x_2 + \dots + w_n x_n$$

根据统计学的方差性质（假设 $w$ 和 $x$ 独立，且均值为 0）：

**和的方差 = 方差的和**

$$Var(y) = Var(w_1 x_1) + Var(w_2 x_2) + \dots + Var(w_n x_n)$$

再根据乘积的方差性质（$Var(AB) \approx Var(A)Var(B)$）：

$$Var(y) = n \times Var(w) \times Var(x)$$

**关键公式来了！**

为了让输入输出方差相等，也就是 $Var(y) = Var(x)$，我们必须保证中间这一坨等于 1：

$$n \times Var(w) = 1$$

所以，权重的方差 $Var(w)$ 必须等于：

$$Var(w) = \frac{1}{n}$$

- 这里的 $n$ 就是输入神经元的个数（**fan_in**）。
- 这就是 **Xavier 初始化** 的基本公式由来！它告诉我们，权重不能瞎随机，必须服从方差为 $1/n$ 的分布。

------

3. 为什么要区分 Xavier 和 Kaiming？

既然公式 $Var(w) = 1/n$ 这么完美，为什么后来 Kaiming (何恺明) 还要改它？

**因为激活函数（Activation Function）是个捣乱分子。**

上面的推导假设了 $y = wx$（线性）。但实际上我们有激活函数 $y = \sigma(wx)$。

- **对于 Tanh / Sigmoid (Xavier 适用)**：

  它们在 0 附近近似是线性的（导数为1）。所以 $Var(w) = 1/n$ 依然大致成立。

- **对于 ReLU (Kaiming 适用)**：

  ReLU 函数是 $y = \max(0, x)$。

  它把所有小于 0 的数都变成了 0。这相当于**把一半的信号给杀死了**！

  - 物理直觉：你砍掉了一半的信号，剩下的信号方差（能量）自然就减半了。

  - 所以，输出方差变成了原来的一半：

    $$Var(y) = \frac{1}{2} \times n \times Var(w) \times Var(x)$$

  为了把这一半补回来，我们需要把权重的方差**乘以 2**：

  $$n \times Var(w) = 2 \quad \Rightarrow \quad Var(w) = \frac{2}{n}$$

  这就是 **Kaiming 初始化** 的由来。那个分子上的 **2**，就是为了抵消 ReLU 带来的“能量损失”。

**“方差保持”** 是深度学习能训练 **深层** 网络的基石。

1. **目的**：保证数据流（前向）和梯度流（反向）在网络中畅通无阻，不会因为层数变多而消失或爆炸。
2. **手段**：通过数学计算，设定初始权重的**随机分布范围**。
3. **直觉**：
   - 如果你用 **Tanh**，用 Xavier 初始化（因为 Tanh 比较“温和”）。
   - 如果你用 **ReLU**，用 Kaiming 初始化（因为 ReLU 会“杀掉”一半方差，要加倍补回来）。

这就好比你传话游戏，如果中间每个人都容易漏听一半的话（ReLU），那你一开始说话的时候就要**嗓门大一倍**（Kaiming Init），这样传到最后才能听得清。

## 补充一下神经网络和神经元

### 第一部分：神经元 (Neuron) —— 最小的决策单元

神经网络之所以叫这个名字，是因为它模仿了人类大脑的**神经元**。

1. 生物学上的样子

你可以把一个生物神经元想象成一个**“八卦传播者”**：

1. **树突 (Dendrites)**：它的耳朵。负责从周围几十个其他神经元那里听八卦（接收电信号）。
2. **细胞体 (Soma)**：它的大脑。负责汇总这些八卦，看看劲爆不劲爆。
3. **轴突 (Axon)**：它的嘴巴。如果八卦够劲爆（超过阈值），它就产生一个电脉冲（点火/Firing），把消息传给下一个人。

2. 数学上的样子 (Perceptron)

在计算机里，我们把这个生物过程抽象成了一个数学公式。这就是一个**人工神经元**。

它由三部分组成，就像我们在做一个**“是否买这件衣服”**的决定：

- **输入 (Inputs, $x$)**：影响决定的因素。
  - $x_1$: 价格便宜吗？
  - $x_2$: 款式好看吗？
  - $x_3$: 牌子好吗？
- **权重 (Weights, $w$)**：你对每个因素的重视程度。
  - $w_1 = 5$ (我很穷，价格最重要)
  - $w_2 = 2$ (款式凑合就行)
  - $w_3 = 0.1$ (不在乎牌子)
- **偏置 (Bias, $b$)**：你的基础门槛（比如你今天心情好不好，或者本身就有购物欲）。
  - $b = -10$ (我本来不想买，门槛很高)

**决策过程（计算过程）：**

$$决定值 = (价格 \times 5) + (款式 \times 2) + (牌子 \times 0.1) + (-10)$$

- **激活函数 (Activation)**：这就是那个**开关**。
  - 如果算出来的 `决定值 > 0` $\rightarrow$ 输出 1 (买！激活！)
  - 如果算出来的 `决定值 \le 0` $\rightarrow$ 输出 0 (不买！不激活！)

**一句话总结：神经元就是一个带有“偏见”（权重）的加权求和器，后面跟个“开关”（激活函数）。**

### 第二部分：神经网络 (Neural Network) —— 决策委员会

一个神经元能做的事太简单了（只能画一条直线）。但如果我们把**成千上万个神经元**连在一起，奇迹就发生了。

这就是**神经网络**。

你可以把它想象成一个**层级分明的公司**：

1. 输入层 (Input Layer) —— 实习生

- **任务**：只负责接收原材料（数据）。
- **你的电池项目**：输入层有 3 个神经元，分别接收 [电压, 电流, 温度]。它们不思考，直接把数据往后传。

2. 隐藏层 (Hidden Layers) —— 中层管理者

这是最关键的地方！也就是通常说的“黑盒”。

- **第一层隐藏层**：负责从原始数据里找**简单规律**。
  - 神经元 A 发现：“哎？电压突然降得很厉害。”
  - 神经元 B 发现：“温度有点高。”
- **第二层隐藏层**：负责**综合**第一层的信息，找**复杂规律**。
  - 神经元 C 听了 A 和 B 的汇报，总结出：“电压降+高温 = 可能是**内阻变大**了！”
- **第三层隐藏层**：更抽象的理解。
  - 神经元 D 总结：“内阻变大通常意味着**电池老化严重**。”

3. 输出层 (Output Layer) —— CEO

- **任务**：根据最后那几个高层经理（隐藏层神经元）的汇报，给出最终结论。
- **结论**：CEO 拍板说：“预测 SOH = 0.82。”

### 第三部分：核心逻辑 —— 它们是怎么“变聪明”的？

你可能会问：*“那个神经元 A 怎么知道它负责看‘电压下降’？神经元 B 怎么知道它负责看‘温度’？”*

**没人告诉它们！** 这一点至关重要。

在最开始（初始化）的时候，所有神经元的权重（$w$）都是随机生成的垃圾数字。这家公司里全是“傻子”，谁也不懂业务。

**变聪明的过程（训练 Training）：**

1. **瞎猜**：把电池数据丢进去，CEO 瞎猜说 SOH 是 0.5。

2. **被打脸**：真实标签是 0.9。误差（Loss）巨大。

3. **反向传播 (Backpropagation)**：

   - 因为猜错了，董事会（Loss Function）开始一级一级向下**问责**。
   - CEO 说：“是经理 D 告诉我的！” $\rightarrow$ D 的权重被修改。
   - 经理 D 说：“是组长 C 的锅！” $\rightarrow$ C 的权重被修改。
   - ...一直追责到底层。

4. **分工涌现**：

   经过几万次这样的“猜错-问责-修改”循环后，每个神经元为了少挨骂，**自动**学会了特定的技能。

   - 有的神经元发现：只要我盯着“电压斜率”，就能减少挨骂。于是它就变成了“电压斜率检测器”。
   - 有的神经元发现：只要我盯着“温度”，也能立功。于是它就变成了“温度检测器”。

这就是神经网络神奇的地方：**不需要人类教它特征，它自己在巨大的数据压力下，进化出了识别特征的能力。**

# 2026.2.19

## 损失函数

损失函数：衡量模型输出与真实标签的差异

# 2026.2.20

休

# 2026.2.21

休
